{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy Channel Model Implementation for Machine Translation\n",
    "\n",
    "#### Based on Research Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test sentences:\n",
      "Resumption of the session\n",
      "I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
      "In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
      "Please rise, then, for this minute' s silence.\n",
      "(The House rose and observed a minute' s silence)\n",
      "Madam President, on a point of order.\n",
      "You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n",
      "One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\n"
     ]
    }
   ],
   "source": [
    "# Load a few English sentences (source for translation)\n",
    "test_file = r\"D:\\NLP Nosiy Machine Translation\\datasets\\training-parallel-europarl-v7\\training\\europarl-v7.de-en.en\"  # adjust the path if necessary\n",
    "with open(test_file, encoding=\"utf-8\") as f:\n",
    "    test_sentences = [line.strip() for line in f.readlines()[:10]]  # first 10 sentences\n",
    "\n",
    "print(\"Loaded test sentences:\")\n",
    "for s in test_sentences:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models.transformer import TransformerModel\n",
    "\n",
    "direct_model_path = r\"D:\\NLP Nosiy Machine Translation\\transformers_large_models\\wmt19(eng to german)\\wmt19.en-de.joined-dict.ensemble\"\n",
    "direct_model_bpe_codes = r\":\\NLP Nosiy Machine Translation\\transformers_large_models\\wmt19(eng to german)\\wmt19.en-de.joined-dict.ensemble\\bpecodes\"\n",
    "\n",
    "channel_model_path = r\"D:\\NLP Nosiy Machine Translation\\transformers_large_models\\wmt19(german to english)\\wmt19.de-en.joined-dict.ensemble\"\n",
    "channel_model_bpe_codes = r\"D:\\NLP Nosiy Machine Translation\\transformers_large_models\\wmt19(german to english)\\wmt19.de-en.joined-dict.ensemble\\bpecodes\"\n",
    "\n",
    "# Load the direct model (English→German)\n",
    "direct = TransformerModel.from_pretrained(\n",
    "    direct_model_path,\n",
    "    checkpoint_file='model1.pt:model2.pt:model3.pt:model4.pt',  # ensemble checkpoints separated by \":\"\n",
    "    data_name_or_path=direct_model_path,\n",
    "    bpe='fastbpe',\n",
    "    bpe_codes=direct_model_bpe_codes\n",
    ")\n",
    "\n",
    "# Load the channel model (German→English)\n",
    "channel = TransformerModel.from_pretrained(\n",
    "    channel_model_path,\n",
    "    checkpoint_file='model.pt',\n",
    "    data_name_or_path=channel_model_path,\n",
    "    bpe='fastbpe',\n",
    "    bpe_codes=channel_model_bpe_codes\n",
    ")\n",
    "\n",
    "# # Load the direct model (English→German)\n",
    "# direct = TransformerModel.from_pretrained(\n",
    "#     'models/direct/wmt19.en-de.joined-dict.ensemble',\n",
    "#     checkpoint_file='model1.pt:model2.pt:model3.pt:model4.pt',  # ensemble checkpoints separated by \":\"\n",
    "#     data_name_or_path='models/direct/wmt19.en-de.joined-dict.ensemble',\n",
    "#     bpe='fastbpe',\n",
    "#     bpe_codes='models/direct/wmt19.en-de.joined-dict.ensemble/bpecodes'\n",
    "# )\n",
    "\n",
    "# # Load the channel model (German→English)\n",
    "# channel = TransformerModel.from_pretrained(\n",
    "#     'models/channel/wmt19.de-en.joined-dict.single_model',\n",
    "#     checkpoint_file='model.pt',\n",
    "#     data_name_or_path='models/channel/wmt19.de-en.joined-dict.single_model',\n",
    "#     bpe='fastbpe',\n",
    "#     bpe_codes='models/channel/wmt19.de-en.joined-dict.single_model/bpecodes'\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run simple implementation of noisy channel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "def gpt2_score(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    log_likelihood = -outputs.loss.item()\n",
    "    return log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def direct_translate(sentence, beam_size=5):\n",
    "    \"\"\"Generate a translation using the direct model (P(y|x)).\"\"\"\n",
    "    return direct.translate(sentence, beam=beam_size)\n",
    "\n",
    "def noisy_channel_decode(sentence, beam_size=5, lambda_=1.0, mu=1.0):\n",
    "    \"\"\"\n",
    "    Generate translation candidates with the direct model, then re-rank using\n",
    "    the channel model (P(x|y)) and language model (P(y)).\n",
    "    Final score: direct_score + lambda_ * channel_score + mu * lm_score.\n",
    "    \"\"\"\n",
    "    # Generate candidate translations from the direct model\n",
    "    candidates = direct.translate(sentence, beam=beam_size)\n",
    "    if isinstance(candidates, str):\n",
    "        candidates = [candidates]\n",
    "    \n",
    "    # Compute scores for each candidate\n",
    "    direct_scores = [direct.score(candidate) for candidate in candidates]\n",
    "    channel_scores = [channel.score(candidate, sentence) for candidate in candidates]\n",
    "    lm_scores = [gpt2_score(candidate) for candidate in candidates]\n",
    "    \n",
    "    # Calculate final scores for each candidate\n",
    "    final_scores = [d + lambda_ * ch + mu * l for d, ch, l in zip(direct_scores, channel_scores, lm_scores)]\n",
    "    best_idx = np.argmax(final_scores)\n",
    "    return candidates[best_idx]\n",
    "\n",
    "# Example usage on a single sentence:\n",
    "example_sentence = test_sentences[0]\n",
    "print(\"Source:\", example_sentence)\n",
    "print(\"Direct Translation:\", direct_translate(example_sentence))\n",
    "print(\"Noisy Channel Translation:\", noisy_channel_decode(example_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Translations for Test Sentences ===\")\n",
    "for src in test_sentences:\n",
    "    direct_out = direct_translate(src)\n",
    "    noisy_out = noisy_channel_decode(src)\n",
    "    print(\"Source:\", src)\n",
    "    print(\"Direct:\", direct_out)\n",
    "    print(\"Noisy Channel:\", noisy_out)\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# Load reference translations (German sentences)\n",
    "ref_file = \"data/europarl-v7.de-en.de\"  # adjust the path if necessary\n",
    "with open(ref_file, encoding=\"utf-8\") as f:\n",
    "    references = [line.strip() for line in f.readlines()[:10]]\n",
    "\n",
    "# Generate hypothesis translations using the noisy channel decoder\n",
    "hypotheses = [noisy_channel_decode(src) for src in test_sentences]\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu = sacrebleu.corpus_bleu(hypotheses, [references])\n",
    "print(\"BLEU score:\", bleu.score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairseq-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
